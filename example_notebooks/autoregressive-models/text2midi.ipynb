{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "data = {\n",
    "    \"canonical_composer\": [\n",
    "        \"Johann Sebastian Bach\",\n",
    "        \"Ludwig van Beethoven\",\n",
    "        \"Frédéric Chopin\",\n",
    "        \"Wolfgang Amadeus Mozart\",\n",
    "        \"Claude Debussy\",\n",
    "    ],\n",
    "    \"canonical_title\": [\n",
    "        \"Prelude in C Major, BWV 846\",\n",
    "        \"Piano Sonata No. 14 in C-sharp minor, Op. 27 No. 2 (Moonlight Sonata)\",\n",
    "        \"Nocturne in E-flat major, Op. 9 No. 2\",\n",
    "        \"Piano Sonata No. 11 in A major, K. 331 (Rondo Alla Turca)\",\n",
    "        \"Clair de Lune, Suite bergamasque, L. 75\",\n",
    "    ],\n",
    "    \"split\": [\"train\", \"train\", \"train\", \"test\", \"test\"],\n",
    "    \"year\": [2018, 2018, 2018, 2019, 2019],\n",
    "    \"text_annotation\": [\n",
    "        \"A soothing and calming piece by Bach, featuring a serene melody.\",\n",
    "        \"A dramatic and emotional piano sonata by Beethoven, known for its contrasting movements.\",\n",
    "        \"A gentle and lyrical nocturne by Chopin, featuring a beautiful melody.\",\n",
    "        \"A playful and energetic piano sonata by Mozart, known for its catchy Rondo Alla Turca.\",\n",
    "        \"A dreamy and evocative piano piece by Debussy, capturing the beauty of moonlight.\",\n",
    "    ],\n",
    "    \"midi_path\": [\n",
    "        \"midi_files/bach_prelude_in_c_major.midi\",\n",
    "        \"midi_files/beethoven_moonlight_sonata.midi\",\n",
    "        \"midi_files/chopin_nocturne_op9_no2.midi\",\n",
    "        \"midi_files/mozart_piano_sonata_k331.midi\",\n",
    "        \"midi_files/debussy_clair_de_lune.midi\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "outputs": [
    {
     "data": {
      "text/plain": "        canonical_composer                                    canonical_title  \\\n0    Johann Sebastian Bach                        Prelude in C Major, BWV 846   \n1     Ludwig van Beethoven  Piano Sonata No. 14 in C-sharp minor, Op. 27 N...   \n2          Frédéric Chopin              Nocturne in E-flat major, Op. 9 No. 2   \n3  Wolfgang Amadeus Mozart  Piano Sonata No. 11 in A major, K. 331 (Rondo ...   \n4           Claude Debussy            Clair de Lune, Suite bergamasque, L. 75   \n\n   split  year                                    text_annotation  \\\n0  train  2018  A soothing and calming piece by Bach, featurin...   \n1  train  2018  A dramatic and emotional piano sonata by Beeth...   \n2  train  2018  A gentle and lyrical nocturne by Chopin, featu...   \n3   test  2019  A playful and energetic piano sonata by Mozart...   \n4   test  2019  A dreamy and evocative piano piece by Debussy,...   \n\n                                    midi_path  \n0     midi_files/bach_prelude_in_c_major.midi  \n1  midi_files/beethoven_moonlight_sonata.midi  \n2     midi_files/chopin_nocturne_op9_no2.midi  \n3    midi_files/mozart_piano_sonata_k331.midi  \n4       midi_files/debussy_clair_de_lune.midi  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>canonical_composer</th>\n      <th>canonical_title</th>\n      <th>split</th>\n      <th>year</th>\n      <th>text_annotation</th>\n      <th>midi_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Johann Sebastian Bach</td>\n      <td>Prelude in C Major, BWV 846</td>\n      <td>train</td>\n      <td>2018</td>\n      <td>A soothing and calming piece by Bach, featurin...</td>\n      <td>midi_files/bach_prelude_in_c_major.midi</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ludwig van Beethoven</td>\n      <td>Piano Sonata No. 14 in C-sharp minor, Op. 27 N...</td>\n      <td>train</td>\n      <td>2018</td>\n      <td>A dramatic and emotional piano sonata by Beeth...</td>\n      <td>midi_files/beethoven_moonlight_sonata.midi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Frédéric Chopin</td>\n      <td>Nocturne in E-flat major, Op. 9 No. 2</td>\n      <td>train</td>\n      <td>2018</td>\n      <td>A gentle and lyrical nocturne by Chopin, featu...</td>\n      <td>midi_files/chopin_nocturne_op9_no2.midi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wolfgang Amadeus Mozart</td>\n      <td>Piano Sonata No. 11 in A major, K. 331 (Rondo ...</td>\n      <td>test</td>\n      <td>2019</td>\n      <td>A playful and energetic piano sonata by Mozart...</td>\n      <td>midi_files/mozart_piano_sonata_k331.midi</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Claude Debussy</td>\n      <td>Clair de Lune, Suite bergamasque, L. 75</td>\n      <td>test</td>\n      <td>2019</td>\n      <td>A dreamy and evocative piano piece by Debussy,...</td>\n      <td>midi_files/debussy_clair_de_lune.midi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "def extract_piano_roll(midi_path, fs=100):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "\n",
    "    # Extract the piano roll\n",
    "    piano_roll = midi_data.get_piano_roll(fs=fs)\n",
    "\n",
    "    # Transpose the piano roll to have time steps on axis 1\n",
    "    piano_roll = piano_roll.T\n",
    "\n",
    "    return piano_roll\n",
    "\n",
    "train_midi_paths = train_df[\"midi_path\"].tolist()\n",
    "test_midi_paths = test_df[\"midi_path\"].tolist()\n",
    "\n",
    "train_piano_rolls = [extract_piano_roll(midi_path) for midi_path in train_midi_paths]\n",
    "test_piano_rolls = [extract_piano_roll(midi_path) for midi_path in test_midi_paths]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12403456 into shape (128,1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[717], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m target_shape \u001B[38;5;241m=\u001B[39m (num_pitches, \u001B[38;5;28mmin\u001B[39m(max_time_steps, num_time_steps))\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Reshape the piano rolls to have the target shape\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m train_midi_features \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mreshape(piano_roll[:, :target_shape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39mnum_pitches]\u001B[38;5;241m.\u001B[39mT, target_shape) \u001B[38;5;28;01mfor\u001B[39;00m piano_roll \u001B[38;5;129;01min\u001B[39;00m train_piano_rolls_padded]\n\u001B[1;32m     22\u001B[0m test_midi_features \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mreshape(piano_roll[:, :target_shape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39mnum_pitches]\u001B[38;5;241m.\u001B[39mT, target_shape) \u001B[38;5;28;01mfor\u001B[39;00m piano_roll \u001B[38;5;129;01min\u001B[39;00m test_piano_rolls_padded]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Convert to tensors\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[717], line 21\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     18\u001B[0m target_shape \u001B[38;5;241m=\u001B[39m (num_pitches, \u001B[38;5;28mmin\u001B[39m(max_time_steps, num_time_steps))\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Reshape the piano rolls to have the target shape\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m train_midi_features \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mreshape(piano_roll[:, :target_shape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39mnum_pitches]\u001B[38;5;241m.\u001B[39mT, target_shape) \u001B[38;5;28;01mfor\u001B[39;00m piano_roll \u001B[38;5;129;01min\u001B[39;00m train_piano_rolls_padded]\n\u001B[1;32m     22\u001B[0m test_midi_features \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mreshape(piano_roll[:, :target_shape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39mnum_pitches]\u001B[38;5;241m.\u001B[39mT, target_shape) \u001B[38;5;28;01mfor\u001B[39;00m piano_roll \u001B[38;5;129;01min\u001B[39;00m test_piano_rolls_padded]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Convert to tensors\u001B[39;00m\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mreshape\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/numpy/core/fromnumeric.py:298\u001B[0m, in \u001B[0;36mreshape\u001B[0;34m(a, newshape, order)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_reshape_dispatcher)\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreshape\u001B[39m(a, newshape, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m    Gives a new shape to an array without changing its data.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;124;03m           [5, 6]])\u001B[39;00m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreshape\u001B[39m\u001B[38;5;124m'\u001B[39m, newshape, order\u001B[38;5;241m=\u001B[39morder)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[0;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[0;31mValueError\u001B[0m: cannot reshape array of size 12403456 into shape (128,1)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def pad_piano_roll(piano_roll, max_length):\n",
    "    padded_piano_roll = np.pad(piano_roll, ((0, 0), (0, max_length - piano_roll.shape[1])), mode='constant')\n",
    "    return padded_piano_roll\n",
    "\n",
    "# Find the maximum number of time steps across all piano rolls\n",
    "max_length = max([piano_roll.shape[1] for piano_roll in train_piano_rolls + test_piano_rolls])\n",
    "\n",
    "# Pad the piano rolls to have the same number of time steps\n",
    "train_piano_rolls_padded = [pad_piano_roll(piano_roll, max_length) for piano_roll in train_piano_rolls]\n",
    "test_piano_rolls_padded = [pad_piano_roll(piano_roll, max_length) for piano_roll in test_piano_rolls]\n",
    "\n",
    "# Set the target shape to have the maximum number of pitches and a fixed number of time steps\n",
    "num_time_steps = 100\n",
    "num_pitches = 128\n",
    "max_time_steps = math.floor(max_length/num_pitches)\n",
    "target_shape = (num_pitches, min(max_time_steps, num_time_steps))\n",
    "\n",
    "# Reshape the piano rolls to have the target shape\n",
    "train_midi_features = [np.reshape(piano_roll[:, :target_shape[1]*num_pitches].T, target_shape) for piano_roll in train_piano_rolls_padded]\n",
    "test_midi_features = [np.reshape(piano_roll[:, :target_shape[1]*num_pitches].T, target_shape) for piano_roll in test_piano_rolls_padded]\n",
    "\n",
    "# Convert to tensors\n",
    "train_midi_features = torch.tensor(train_midi_features, dtype=torch.float)\n",
    "test_midi_features = torch.tensor(test_midi_features, dtype=torch.float)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "\n",
    "# Load or train the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(vocab=\"tokenizer/vocab.json\", merges=\"tokenizer/merges.txt\")\n",
    "\n",
    "# Define the tokenizer components\n",
    "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "vocab_size = 30000\n",
    "\n",
    "# Save text data to a file\n",
    "with open(\"text_data.txt\", \"w\") as f:\n",
    "    for line in data['text_annotation']:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# Train the tokenizer using the text data file\n",
    "tokenizer.train(files=[\"text_data.txt\"], vocab_size=vocab_size, min_frequency=2, special_tokens=special_tokens)\n",
    "\n",
    "# Add the BertProcessing component to the tokenizer\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "\n",
    "# Save the tokenizer files (optional)\n",
    "if not os.path.exists(\"tokenizer\"):\n",
    "    os.makedirs(\"tokenizer\")\n",
    "tokenizer.save(\"tokenizer/tokenizer.json\")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer/tokenizer.json\")\n",
    "tokenizer.pad_token = \"[PAD]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the text annotations\n",
    "def encode_text(text, tokenizer):\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "class TextMIDIDataset(Dataset):\n",
    "    def __init__(self, texts, midi_features, tokenizer):\n",
    "        self.texts = [torch.tensor(encode_text(text, tokenizer), dtype=torch.long) for text in texts]\n",
    "        self.midi_features = midi_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        midi_feature = self.midi_features[idx]\n",
    "        attention_mask = torch.ones_like(text)\n",
    "        return midi_feature, text, attention_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare your data\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "test_df = df[df[\"split\"] == \"test\"]\n",
    "# Load and process your MIDI files here\n",
    "# For demonstration purposes, we'll create dummy data as placeholders for the actual MIDI features\n",
    "# TODO: Replace this with your own MIDI feature extraction code - prettymidi or music21\n",
    "\n",
    "train_texts = train_df[\"text_annotation\"].tolist()\n",
    "test_texts = test_df[\"text_annotation\"].tolist()\n",
    "\n",
    "train_dataset = TextMIDIDataset(train_texts, train_midi_features, tokenizer)\n",
    "test_dataset = TextMIDIDataset(test_texts, test_midi_features, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, piano_roll in enumerate(train_piano_rolls):\n",
    "    non_zero_count = np.count_nonzero(piano_roll)\n",
    "    total_elements = piano_roll.shape[0] * piano_roll.shape[1]\n",
    "    sparsity = 1 - non_zero_count / total_elements\n",
    "    print(f\"Piano roll {idx}: sparsity = {sparsity * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_piano_roll(piano_roll, title):\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    ax.imshow(piano_roll, aspect=\"auto\", cmap=\"binary_r\", origin=\"lower\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(\"Pitch\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first piano roll in the train set\n",
    "visualize_piano_roll(train_piano_rolls[0], \"Piano Roll for Train MIDI 0\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "class TextToMIDIModel(nn.Module):\n",
    "    def __init__(self, text_vocab_size, midi_feature_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super(TextToMIDIModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(text_vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, midi_feature_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Clip the input_ids to be within the range of the embedding layer\n",
    "        input_ids = input_ids.clamp(max=self.embedding.num_embeddings - 1)\n",
    "\n",
    "        # Convert attention_mask to boolean tensor and transpose it\n",
    "        attention_mask = (attention_mask != 0).transpose(0, 1)\n",
    "\n",
    "        # Add a dimension of size 1 to the attention_mask tensor\n",
    "        attention_mask = attention_mask.unsqueeze(2)\n",
    "\n",
    "        # Use the input_ids for the embedding layer\n",
    "        x = self.embedding(input_ids.long()) * np.sqrt(input_ids.size(1))\n",
    "\n",
    "        # Apply the transformer\n",
    "        output = self.transformer(x, x, src_key_padding_mask=attention_mask, tgt_key_padding_mask=attention_mask)\n",
    "\n",
    "        # Apply the linear layer and remove any extra dimensions\n",
    "        output = self.fc(output).squeeze(dim=0)\n",
    "\n",
    "        # Return the output\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class TextToMIDIModel(nn.Module):\n",
    "    def __init__(self, text_vocab_size, midi_feature_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super(TextToMIDIModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(text_vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, midi_feature_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Clip the input_ids to be within the range of the embedding layer\n",
    "        input_ids = input_ids.clamp(max=self.embedding.num_embeddings - 1)\n",
    "\n",
    "        # Convert attention_mask to boolean tensor and transpose it\n",
    "        attention_mask = (attention_mask != 0).transpose(0, 1)\n",
    "\n",
    "        # Use the input_ids for the embedding layer\n",
    "        x = self.embedding(input_ids.long()) * np.sqrt(input_ids.size(1))\n",
    "\n",
    "        # Add a dimension of size 1 to the attention_mask tensor\n",
    "        attention_mask = attention_mask.unsqueeze(2)\n",
    "\n",
    "        # Apply the transformer\n",
    "        output = self.transformer(x.permute(1, 0, 2), attention_mask=attention_mask)\n",
    "\n",
    "        # Transpose the output back to the original shape\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        # Apply the linear layer and return the output\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set your text vocabulary size and MIDI feature size\n",
    "text_vocab_size = 100  # For example, if you have 10,000 unique words in your text data\n",
    "midi_feature_size = 100  # For example, if you're using a piano roll with 128 notes\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextToMIDIModel(text_vocab_size, midi_feature_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training configurations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "num_epochs = 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for input_ids, attention_mask, midi in dataloader:\n",
    "        print(input_ids.shape)\n",
    "        input_ids, attention_mask, midi = input_ids.to(device), attention_mask.to(device), midi.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Check dimensions of tensors\n",
    "        print(f\"outputs.shape: {outputs.shape}\")\n",
    "        print(f\"midi.shape: {midi.shape}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, midi)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text_data_iterator(text_data):\n",
    "    for line in text_data:\n",
    "        yield line\n",
    "\n",
    "# Example text data\n",
    "\"\"\"\n",
    "text_data = [\n",
    "    \"A deeply emotional and introspective piece...\",\n",
    "    \"A lively and energetic composition...\",\n",
    "    # ... more lines of text data ...\n",
    "]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate_fn(batch):\n",
    "    midi_features, texts, attention_masks = zip(*batch)\n",
    "    midi_features = torch.stack(midi_features)\n",
    "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    return midi_features, texts, attention_masks\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_midi_paths)\n",
    "midi_data = [extract_piano_roll(midi_path) for midi_path in train_midi_paths]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(midi_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (midi, input_ids, attention_mask) in enumerate(dataloader):\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        midi = midi.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, midi)\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / (batch_idx + 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {train_loss:.6f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = eval(model, test_dataloader, criterion, device)\n",
    "        print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Save the model checkpoint after each epoch\n",
    "    checkpoint_path = f\"model_checkpoint_epoch_{epoch + 1}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}