{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caslabs/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example dataset (text descriptions mapped to chord sequences)\n",
    "data = [\n",
    "    ('an evil sinister uprising', 'Cm Gm Bb Fm'),\n",
    "    ('a happy sunny day', 'C F G C'),\n",
    "    ('a sorrowful and emotional moment', 'Am Dm F E7'),\n",
    "]\n",
    "\n",
    "# Split the dataset into input and output pairs\n",
    "input_texts, output_chords = zip(*data)\n",
    "\n",
    "# Tokenize the input and output sequences\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "input_tokens = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "output_tokens = tokenizer(output_chords, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cm Gm Bb Fm', 'C F G C', 'Am Dm F E7')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   46,  8293,  3731,  5805,    95,    52,  4890,     1],\n",
       "        [    3,     9,  1095, 13546,   239,     1,     0,     0],\n",
       "        [    3,     9, 27225,  1329,    11,  3973,   798,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m dot_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel.dot\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([\u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)  \u001b[39m# Create a dummy input tensor\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mexport(model, dummy_input, dot_file, opset_version\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel graph saved to \u001b[39m\u001b[39m{\u001b[39;00mdot_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Convert the dot file to an image (requires Graphviz)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     _export(\n\u001b[1;32m    507\u001b[0m         model,\n\u001b[1;32m    508\u001b[0m         args,\n\u001b[1;32m    509\u001b[0m         f,\n\u001b[1;32m    510\u001b[0m         export_params,\n\u001b[1;32m    511\u001b[0m         verbose,\n\u001b[1;32m    512\u001b[0m         training,\n\u001b[1;32m    513\u001b[0m         input_names,\n\u001b[1;32m    514\u001b[0m         output_names,\n\u001b[1;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39moperator_export_type,\n\u001b[1;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39mopset_version,\n\u001b[1;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39mdo_constant_folding,\n\u001b[1;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39mdynamic_axes,\n\u001b[1;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39mkeep_initializers_as_inputs,\n\u001b[1;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39mcustom_opsets,\n\u001b[1;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39mexport_modules_as_functions,\n\u001b[1;32m    522\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1549\u001b[0m     model,\n\u001b[1;32m   1550\u001b[0m     args,\n\u001b[1;32m   1551\u001b[0m     verbose,\n\u001b[1;32m   1552\u001b[0m     input_names,\n\u001b[1;32m   1553\u001b[0m     output_names,\n\u001b[1;32m   1554\u001b[0m     operator_export_type,\n\u001b[1;32m   1555\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1556\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39mfixed_batch_size,\n\u001b[1;32m   1557\u001b[0m     training\u001b[39m=\u001b[39mtraining,\n\u001b[1;32m   1558\u001b[0m     dynamic_axes\u001b[39m=\u001b[39mdynamic_axes,\n\u001b[1;32m   1559\u001b[0m )\n\u001b[1;32m   1561\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:989\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    984\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    985\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    990\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    991\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:893\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    891\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    892\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 893\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_get_trace_graph(\n\u001b[1;32m    894\u001b[0m     model,\n\u001b[1;32m    895\u001b[0m     args,\n\u001b[1;32m    896\u001b[0m     strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    897\u001b[0m     _force_outplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    898\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    899\u001b[0m )\n\u001b[1;32m    900\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    902\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/jit/_trace.py:1268\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1267\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1268\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1269\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner(\u001b[39m*\u001b[39mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1716\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[1;32m   1717\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1718\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1719\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1720\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1721\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m   1722\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1723\u001b[0m     head_mask\u001b[39m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1724\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1725\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1726\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1727\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1728\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1729\u001b[0m )\n\u001b[1;32m   1731\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:981\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    980\u001b[0m     err_msg_prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minput_ids or \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    984\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Config, AdamW\n",
    "import torch\n",
    "\n",
    "config = T5Config.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', config=config)\n",
    "\n",
    "# Generate the model graph\n",
    "dot_file = 'model.dot'\n",
    "dummy_input = torch.zeros([1, 512], dtype=torch.long)  # Create a dummy input tensor\n",
    "torch.onnx.export(model, dummy_input, dot_file, opset_version=11, verbose=True)\n",
    "print(f'Model graph saved to {dot_file}')\n",
    "\n",
    "# Convert the dot file to an image (requires Graphviz)\n",
    "image_file = 'model.png'\n",
    "!dot -Tpng {dot_file} -o {image_file}\n",
    "print(f'Model image saved to {image_file}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "input_tokens.to(device)\n",
    "output_tokens.to(device)\n",
    "\n",
    "# Training settings\n",
    "epochs = 30\n",
    "batch_size = 1\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Training loop\n",
    "# Modified training loop\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        input_batch = input_tokens['input_ids'][i:i+batch_size].to(device)\n",
    "        output_batch = output_tokens['input_ids'][i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_batch, labels=output_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(input_texts)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n",
    "    loss_values.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \"a romantic evening\" -> Output chords: \"Am Dm F\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caslabs/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "test_input = 'a romantic evening'\n",
    "\n",
    "# Tokenize the test input\n",
    "test_input_tokens = tokenizer(test_input, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output_tokens = model.generate(test_input_tokens['input_ids'], num_return_sequences=1)\n",
    "\n",
    "# Convert output tokens to chords\n",
    "test_output_chords = tokenizer.decode(test_output_tokens[0], skip_special_tokens=True)\n",
    "print(f'Test input: \"{test_input}\" -> Output chords: \"{test_output_chords}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, epochs + 1), loss_values)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: BatchEncoding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Generate the model graph\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dot_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel.dot\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mexport(model, input_tokens, dot_file, opset_version\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel graph saved to \u001b[39m\u001b[39m{\u001b[39;00mdot_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Convert the dot file to an image (requires Graphviz)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     _export(\n\u001b[1;32m    507\u001b[0m         model,\n\u001b[1;32m    508\u001b[0m         args,\n\u001b[1;32m    509\u001b[0m         f,\n\u001b[1;32m    510\u001b[0m         export_params,\n\u001b[1;32m    511\u001b[0m         verbose,\n\u001b[1;32m    512\u001b[0m         training,\n\u001b[1;32m    513\u001b[0m         input_names,\n\u001b[1;32m    514\u001b[0m         output_names,\n\u001b[1;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39moperator_export_type,\n\u001b[1;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39mopset_version,\n\u001b[1;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39mdo_constant_folding,\n\u001b[1;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39mdynamic_axes,\n\u001b[1;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39mkeep_initializers_as_inputs,\n\u001b[1;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39mcustom_opsets,\n\u001b[1;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39mexport_modules_as_functions,\n\u001b[1;32m    522\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1549\u001b[0m     model,\n\u001b[1;32m   1550\u001b[0m     args,\n\u001b[1;32m   1551\u001b[0m     verbose,\n\u001b[1;32m   1552\u001b[0m     input_names,\n\u001b[1;32m   1553\u001b[0m     output_names,\n\u001b[1;32m   1554\u001b[0m     operator_export_type,\n\u001b[1;32m   1555\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1556\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39mfixed_batch_size,\n\u001b[1;32m   1557\u001b[0m     training\u001b[39m=\u001b[39mtraining,\n\u001b[1;32m   1558\u001b[0m     dynamic_axes\u001b[39m=\u001b[39mdynamic_axes,\n\u001b[1;32m   1559\u001b[0m )\n\u001b[1;32m   1561\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:989\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    984\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    985\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    990\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    991\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/onnx/utils.py:893\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    891\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    892\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 893\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_get_trace_graph(\n\u001b[1;32m    894\u001b[0m     model,\n\u001b[1;32m    895\u001b[0m     args,\n\u001b[1;32m    896\u001b[0m     strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    897\u001b[0m     _force_outplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    898\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    899\u001b[0m )\n\u001b[1;32m    900\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    902\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/jit/_trace.py:1268\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1267\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1268\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1269\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/music-gen/lib/python3.11/site-packages/torch/jit/_trace.py:95\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 95\u001b[0m     in_vars, in_desc \u001b[39m=\u001b[39m _flatten(args)\n\u001b[1;32m     96\u001b[0m     \u001b[39m# NOTE: use full state, because we need it for BatchNorm export\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# This differs from the compiler path, which doesn't support it at the moment.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     module_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_unique_state_dict(\u001b[39mself\u001b[39m, keep_vars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: BatchEncoding"
     ]
    }
   ],
   "source": [
    "# Generate the model graph\n",
    "dot_file = 'model.dot'\n",
    "torch.onnx.export(model, input_tokens, dot_file, opset_version=11, verbose=True)\n",
    "print(f'Model graph saved to {dot_file}')\n",
    "\n",
    "# Convert the dot file to an image (requires Graphviz)\n",
    "image_file = 'model.png'\n",
    "!dot -Tpng {dot_file} -o {image_file}\n",
    "print(f'Model image saved to {image_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
